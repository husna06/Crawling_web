{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Crawler Menggunakan Python Nama : Husna NIM : 160411100018 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom Pengantar Program ini menggunakan Bahasa Python dengan library : Beautifulsoup4 requests SQlite3 numpy scipy scikit-learn Mengambil data teks di Website (IMDb Top Box Office) : \" https://jatim.sindonews.com/loadmore/0/ \" from math import log10 import requests from bs4 import BeautifulSoup import sqlite3 from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from Sastrawi.Stemmer.StemmerFactory import StemmerFactory import csv from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np import warnings warnings . filterwarnings ( ignore )","title":"Pengantar"},{"location":"#web-crawler-menggunakan-python","text":"Nama : Husna NIM : 160411100018 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom","title":"Web Crawler Menggunakan Python"},{"location":"#pengantar","text":"Program ini menggunakan Bahasa Python dengan library : Beautifulsoup4 requests SQlite3 numpy scipy scikit-learn Mengambil data teks di Website (IMDb Top Box Office) : \" https://jatim.sindonews.com/loadmore/0/ \" from math import log10 import requests from bs4 import BeautifulSoup import sqlite3 from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from Sastrawi.Stemmer.StemmerFactory import StemmerFactory import csv from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np import warnings warnings . filterwarnings ( ignore )","title":"Pengantar"},{"location":"Crawling/","text":"Crawling Web Crawler digunakan untuk mengambil data berupa teks, audio, foto bahkan video pada sebuah website. source code untuk mengcrawl data Proses crawling dalam suatu website dimulai dari mendata seluruh url dari website, menelusurinya satu-persatu, kemudian memasukkannya dalam daftar halaman pada indeks search engine, sehingga setiap kali ada perubahan pada website, akan terupdate secara otomatis[^1]. src = https://jatim.sindonews.com/loadmore/0/ page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) news = soup . findAll ( class_ = caption ) koneksi = sqlite3 . connect ( berita.db ) koneksi . execute ( CREATE TABLE if not exists berita (NAMA_EVENT VARCHAR NOT NULL, DESKRIPSI VARCHAR NOT NULL); ) for i in range ( len ( news )): link = news [ i ][ href ] page = requests . get ( link ) soup = BeautifulSoup ( page . content , html.parser ) ivent = soup . find ( class_ = tahoma ) . getText () deskripsi = soup . find ( class_ = article col-md-11 ) paragraf = deskripsi . findAll ( p ) p = for a in paragraf : p += a . getText () + cek = koneksi . execute ( SELECT * FROM berita where NAMA_EVENT=? , ( ivent ,)) cek = cek . fetchall () if len ( cek ) == 0 : koneksi . execute ( INSERT INTO berita values (?,?) , ( ivent , p )); koneksi . commit () tampil = koneksi . execute ( SELECT * FROM berita ) with open ( data_crawler.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in tampil : employee_writer . writerow ( i ) tampil = koneksi . execute ( SELECT * FROM berita ) isi = [] for row in tampil : isi . append ( row [ 1 ]) print ( row ) print ( berita ) Data yang telah di crawl kemudian di simpan ke dalam database yaitu SQlite, kita bisa mecari teks yang mau kita ambil pada web tersebut, untuk contoh tersebut kita mengambil Judul Berita dan Isi Berita , rating hasilnya pada hari weekend dan gross , dan untuk memperjelas tempat data yang mau kita ambil pahami terlebih dahulu di html web. disini kita menggunakan link buat halaman web yang akan kita ambil datanya, dibagian src kita mau ngambil judul sedangkan di bagian news kita mau ngambil bagian isinya berita tersebut. Hasil Running :","title":"Crawling"},{"location":"Crawling/#crawling","text":"Web Crawler digunakan untuk mengambil data berupa teks, audio, foto bahkan video pada sebuah website.","title":"Crawling"},{"location":"Crawling/#source-code-untuk-mengcrawl-data","text":"Proses crawling dalam suatu website dimulai dari mendata seluruh url dari website, menelusurinya satu-persatu, kemudian memasukkannya dalam daftar halaman pada indeks search engine, sehingga setiap kali ada perubahan pada website, akan terupdate secara otomatis[^1]. src = https://jatim.sindonews.com/loadmore/0/ page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) news = soup . findAll ( class_ = caption ) koneksi = sqlite3 . connect ( berita.db ) koneksi . execute ( CREATE TABLE if not exists berita (NAMA_EVENT VARCHAR NOT NULL, DESKRIPSI VARCHAR NOT NULL); ) for i in range ( len ( news )): link = news [ i ][ href ] page = requests . get ( link ) soup = BeautifulSoup ( page . content , html.parser ) ivent = soup . find ( class_ = tahoma ) . getText () deskripsi = soup . find ( class_ = article col-md-11 ) paragraf = deskripsi . findAll ( p ) p = for a in paragraf : p += a . getText () + cek = koneksi . execute ( SELECT * FROM berita where NAMA_EVENT=? , ( ivent ,)) cek = cek . fetchall () if len ( cek ) == 0 : koneksi . execute ( INSERT INTO berita values (?,?) , ( ivent , p )); koneksi . commit () tampil = koneksi . execute ( SELECT * FROM berita ) with open ( data_crawler.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in tampil : employee_writer . writerow ( i ) tampil = koneksi . execute ( SELECT * FROM berita ) isi = [] for row in tampil : isi . append ( row [ 1 ]) print ( row ) print ( berita ) Data yang telah di crawl kemudian di simpan ke dalam database yaitu SQlite, kita bisa mecari teks yang mau kita ambil pada web tersebut, untuk contoh tersebut kita mengambil Judul Berita dan Isi Berita , rating hasilnya pada hari weekend dan gross , dan untuk memperjelas tempat data yang mau kita ambil pahami terlebih dahulu di html web. disini kita menggunakan link buat halaman web yang akan kita ambil datanya, dibagian src kita mau ngambil judul sedangkan di bagian news kita mau ngambil bagian isinya berita tersebut.","title":"source code untuk mengcrawl data"},{"location":"Crawling/#hasil-running","text":"","title":"Hasil Running :"},{"location":"Graph/","text":"Graph Graph dapat digunakan untuk merepresentasikan objek-objek diskrit dan hubungan antara objek-objek tersebut. Representasi visual dari graph adalah dengan menyatakan objek sebagai noktah, bulatan atau titik (Vertex), sedangkan hubungan antara objek dinyatakan dengan garis (Edge). G = (V, E) \u200b Dimana \u200b G = Graph \u200b V = Simpul atau Vertex, atau Node, atau Titik \u200b E = Busur atau Edge, atau arc Pada proses graph ini menggunakan library Network as nx kemudian lanjut diproses menggunakan matplotlib.pyplot as plt agar graph mudah dipahami. ```g = nx.from_pandas_edgelist(edgelistFrame, \"From\", \"To\", None, nx.DiGraph()) g = nx.from_pandas_edgelist(edgelistFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() ``` Graf merupakan suatu cabang ilmu yang memiliki banyak terapan. Banyak sekali struktur yang bisa direpresentasikan dengan graf, dan banyak masalah yang bisa diselesaikan dengan bantuan graf. Hasil Running :","title":"Graph"},{"location":"Graph/#graph","text":"Graph dapat digunakan untuk merepresentasikan objek-objek diskrit dan hubungan antara objek-objek tersebut. Representasi visual dari graph adalah dengan menyatakan objek sebagai noktah, bulatan atau titik (Vertex), sedangkan hubungan antara objek dinyatakan dengan garis (Edge). G = (V, E) \u200b Dimana \u200b G = Graph \u200b V = Simpul atau Vertex, atau Node, atau Titik \u200b E = Busur atau Edge, atau arc Pada proses graph ini menggunakan library Network as nx kemudian lanjut diproses menggunakan matplotlib.pyplot as plt agar graph mudah dipahami. ```g = nx.from_pandas_edgelist(edgelistFrame, \"From\", \"To\", None, nx.DiGraph()) g = nx.from_pandas_edgelist(edgelistFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() ``` Graf merupakan suatu cabang ilmu yang memiliki banyak terapan. Banyak sekali struktur yang bisa direpresentasikan dengan graf, dan banyak masalah yang bisa diselesaikan dengan bantuan graf.","title":"Graph"},{"location":"K-Means/","text":"K-Maens K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_","title":"K-means"},{"location":"K-Means/#k-maens","text":"K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_","title":"K-Maens"},{"location":"PageRank/","text":"PageRank PageRank adalah sebuah algoritme yang telah dipatenkan yang berfungsi menentukan situs web mana yang lebih penting/populer. PageRank , memiliki konsep dasar yang sama dengan link popularity , tetapi tidak hanya memperhitungkan \u201cjumlah\u201d inbound dan outbound link . Pendekatan yang digunakan adalah sebuah halaman akan diangap penting jika halaman lain memiliki link ke halaman tersebut. Sebuah halaman juga akan menjadi semakin penting jika halaman lain yang memiliki rangking (pagerank) tinggi mengacu ke halaman tersebut. damping = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping , max_iter = max_iterr , tol = error_toleransi ) # Membuat Label print pagerank print ( keterangan node: ) nodelist = g . nodes label = {} data = [] for i , key in enumerate ( nodelist ): data . append (( pr [ key ], key )) label [ key ] = i #print(i, key, pr[key]) code berikut untuk mengurutkan nilai pagerank dari terbesar hingga terkecil. urut = data . copy () for x in range ( len ( urut )): for y in range ( len ( urut )): if urut [ x ][ 0 ] urut [ y ][ 0 ]: urut [ x ], urut [ y ] = urut [ y ], urut [ x ] urut = pd . DataFrame ( urut , None , ( PageRank , Node )) print ( urut ) Hasil Running :","title":"PageRank"},{"location":"PageRank/#pagerank","text":"PageRank adalah sebuah algoritme yang telah dipatenkan yang berfungsi menentukan situs web mana yang lebih penting/populer. PageRank , memiliki konsep dasar yang sama dengan link popularity , tetapi tidak hanya memperhitungkan \u201cjumlah\u201d inbound dan outbound link . Pendekatan yang digunakan adalah sebuah halaman akan diangap penting jika halaman lain memiliki link ke halaman tersebut. Sebuah halaman juga akan menjadi semakin penting jika halaman lain yang memiliki rangking (pagerank) tinggi mengacu ke halaman tersebut. damping = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping , max_iter = max_iterr , tol = error_toleransi ) # Membuat Label print pagerank print ( keterangan node: ) nodelist = g . nodes label = {} data = [] for i , key in enumerate ( nodelist ): data . append (( pr [ key ], key )) label [ key ] = i #print(i, key, pr[key]) code berikut untuk mengurutkan nilai pagerank dari terbesar hingga terkecil. urut = data . copy () for x in range ( len ( urut )): for y in range ( len ( urut )): if urut [ x ][ 0 ] urut [ y ][ 0 ]: urut [ x ], urut [ y ] = urut [ y ], urut [ x ] urut = pd . DataFrame ( urut , None , ( PageRank , Node )) print ( urut )","title":"PageRank"},{"location":"Silhouette/","text":"Silhouette Silhouette merupakan evaluasi cluster apakah cluster yang digunakan sudah baik apa belum. for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) Hasil Running :","title":"Evaluasi"},{"location":"Silhouette/#silhouette","text":"Silhouette merupakan evaluasi cluster apakah cluster yang digunakan sudah baik apa belum. for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ )","title":"Silhouette"},{"location":"Silhouette/#hasil-running","text":"","title":"Hasil Running :"},{"location":"TF-IDF/","text":"TF-IDF source code tf-idf TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. df = list () for d in range ( len ( matrix [ 0 ])): total = 0 for i in range ( len ( matrix )): if matrix [ i ][ d ] != 0 : total += 1 df . append ( total ) idf = list () for i in df : tmp = 1 + log10 ( len ( matrix ) / ( 1 + i )) idf . append ( tmp ) tf = matrix tfidf = [] for baris in range ( len ( matrix )): tampungBaris = [] for kolom in range ( len ( matrix [ 0 ])): tmp = tf [ baris ][ kolom ] * idf [ kolom ] tampungBaris . append ( tmp ) tfidf . append ( tampungBaris ) tfidf = np . array ( tfidf ) print ( tfidf ) with open ( tf-idf.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in tfidf : employee_writer . writerow ( i ) Untuk menyimpan hasil tf - idf dalam bentuk CSV dan untuk menyeleksi fitur, me - Klustering, Silhoute hasil dati data yang di telah di ambil atau crawl dan yang telah di seleksi menggunakan KBI. kemudian kita hitung kata yang sama dalam 1 dokumen atau 1 link tersebut dengan menggunakan tf - idf Hasil Running :","title":"TF-IDF"},{"location":"TF-IDF/#tf-idf","text":"","title":"TF-IDF"},{"location":"TF-IDF/#source-code-tf-idf","text":"TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. df = list () for d in range ( len ( matrix [ 0 ])): total = 0 for i in range ( len ( matrix )): if matrix [ i ][ d ] != 0 : total += 1 df . append ( total ) idf = list () for i in df : tmp = 1 + log10 ( len ( matrix ) / ( 1 + i )) idf . append ( tmp ) tf = matrix tfidf = [] for baris in range ( len ( matrix )): tampungBaris = [] for kolom in range ( len ( matrix [ 0 ])): tmp = tf [ baris ][ kolom ] * idf [ kolom ] tampungBaris . append ( tmp ) tfidf . append ( tampungBaris ) tfidf = np . array ( tfidf ) print ( tfidf ) with open ( tf-idf.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in tfidf : employee_writer . writerow ( i ) Untuk menyimpan hasil tf - idf dalam bentuk CSV dan untuk menyeleksi fitur, me - Klustering, Silhoute hasil dati data yang di telah di ambil atau crawl dan yang telah di seleksi menggunakan KBI. kemudian kita hitung kata yang sama dalam 1 dokumen atau 1 link tersebut dengan menggunakan tf - idf","title":"source code tf-idf"},{"location":"TF-IDF/#hasil-running","text":"","title":"Hasil Running :"},{"location":"VSM/","text":"VSM source code untuk VSM Vector Space Model (VSM) digunakan sebagai representasi dari kumpulan dataset dokumen teks. Dokumen dalam Vector Space Model (VSM) berupa matriks yang berisi bobot seluruh kata pada tiap dokumen. Bobot tersebut menyatakan kepentingan atau kontribusi kata terhadap suatu dokumen dan kumpulan dokumen. factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () factory = StemmerFactory () stemmer = factory . create_stemmer () tmp = for i in isi : tmp = tmp + + i hasil = [] for i in tmp . split (): try : if i . isalpha () and ( not i in hasil ) and len ( i ) 1 : # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) if stop != : stem = stemmer . stem ( stop ) hasil . append ( stem ) except : continue katadasar = np . array ( hasil ) print ( katadasar ) Untuk memulai proses Stopword Removal pastikan kita menginstall library Sastrawi, silahkan baca tulisan saya tentang instalasi library Python Sastrawi. Selanjutnya adalah mengimport kelas StopWordRemoverFactory dari library sastrawi. source code untuk menyeleksi kata dasar agar sesuai dengan KBBI #KBBI koneksi = sqlite3 . connect ( KBI.db ) cur_kbi = koneksi . execute ( SELECT* from KATA ) def LinearSearch ( kbi , kata ): found = False posisi = 0 while posisi len ( kata ) and not found : if kata [ posisi ] == kbi : found = True posisi = posisi + 1 return found berhasil = [] for kata in cur_kbi : ketemu = LinearSearch ( kata [ 0 ], katadasar ) if ketemu : kata = kata [ 0 ] berhasil . append ( kata ) #print(berhasil) katadasar = np . array ( berhasil ) matrix = [] for row in isi : tamp_isi = [] for a in katadasar : tamp_isi . append ( row . lower () . count ( a )) matrix . append ( tamp_isi ) print ( kbi ) print ( katadasar ) for m in matrix : print ( m ) with open ( data_matrix.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in matrix : employee_writer . writerow ( i ) kata yang sudah terseleksi kemudian di simpan ke dalam csv dalam bentuk matrix. Hasil Running :","title":"VSM"},{"location":"VSM/#vsm","text":"","title":"VSM"},{"location":"VSM/#source-code-untuk-vsm","text":"Vector Space Model (VSM) digunakan sebagai representasi dari kumpulan dataset dokumen teks. Dokumen dalam Vector Space Model (VSM) berupa matriks yang berisi bobot seluruh kata pada tiap dokumen. Bobot tersebut menyatakan kepentingan atau kontribusi kata terhadap suatu dokumen dan kumpulan dokumen. factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () factory = StemmerFactory () stemmer = factory . create_stemmer () tmp = for i in isi : tmp = tmp + + i hasil = [] for i in tmp . split (): try : if i . isalpha () and ( not i in hasil ) and len ( i ) 1 : # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) if stop != : stem = stemmer . stem ( stop ) hasil . append ( stem ) except : continue katadasar = np . array ( hasil ) print ( katadasar ) Untuk memulai proses Stopword Removal pastikan kita menginstall library Sastrawi, silahkan baca tulisan saya tentang instalasi library Python Sastrawi. Selanjutnya adalah mengimport kelas StopWordRemoverFactory dari library sastrawi.","title":"source code untuk VSM"},{"location":"VSM/#source-code-untuk-menyeleksi-kata-dasar-agar-sesuai-dengan-kbbi","text":"#KBBI koneksi = sqlite3 . connect ( KBI.db ) cur_kbi = koneksi . execute ( SELECT* from KATA ) def LinearSearch ( kbi , kata ): found = False posisi = 0 while posisi len ( kata ) and not found : if kata [ posisi ] == kbi : found = True posisi = posisi + 1 return found berhasil = [] for kata in cur_kbi : ketemu = LinearSearch ( kata [ 0 ], katadasar ) if ketemu : kata = kata [ 0 ] berhasil . append ( kata ) #print(berhasil) katadasar = np . array ( berhasil ) matrix = [] for row in isi : tamp_isi = [] for a in katadasar : tamp_isi . append ( row . lower () . count ( a )) matrix . append ( tamp_isi ) print ( kbi ) print ( katadasar ) for m in matrix : print ( m ) with open ( data_matrix.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in matrix : employee_writer . writerow ( i ) kata yang sudah terseleksi kemudian di simpan ke dalam csv dalam bentuk matrix.","title":"source code untuk menyeleksi kata dasar agar sesuai dengan KBBI"},{"location":"VSM/#hasil-running","text":"","title":"Hasil Running :"},{"location":"crawl_link/","text":"Crawling Web Crawler digunakan untuk mengambil data berupa teks, audio, foto bahkan video pada sebuah website. source code untuk mengcrawl data Proses crawling dalam suatu website dimulai dari mendata seluruh url dari website, menelusurinya satu-persatu, kemudian memasukkannya dalam daftar halaman pada indeks search engine, sehingga setiap kali ada perubahan pada website, akan terupdate secara otomatis[^1]. crawling pada halaman ini mengambil link yang ada pada sebuah web , disini saya mengambil link pada web sariayu : \" https://sariayu.com/ \" untuk mengambil link kita bisa menggunakan code dibawah ini yang sudah dijadikan sebagai target dan menambahkan https pada link. def simplifiedURL ( url ): if www. in url : ind = url . index ( www. ) + 4 url = http:// + url [ ind :] if url [ - 1 ] == / : url = url [: - 1 ] parts = url . split ( / ) url = for i in range ( 3 ): url += parts [ i ] + / return url code dibawah ini menggunakan 4 para meter antara lain : deep : mengecek kedalaman crawl show : menampilkan hasil crawl max_deep : batasan kedalaman saat proses crawl url : target alamat yang akan di crawling def crawl ( url , max_deep , show = False , deep = 0 , done = []): global edgelist deep += 1 url = simplifiedURL ( url ) if not url in done : links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( | , end = ) for i in range ( deep - 1 ): print ( -- , end = ) print ( ( %d ) %s % ( len ( links ), url )) for link in links : link = simplifiedURL ( link ) edge = ( url , link ) if not edge in edgelist : edgelist . append ( edge ) if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) selanjutnya menggunakan code dibawah ini untuk mengambil data link pada sebuah web. def getAllLinks ( src ): try : page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) tags = soup . findAll ( a ) links = [] for tag in tags : try : link = tag [ href ] if not link in links and http in link : links . append ( link ) except KeyError : pass return links except : return list () Inisialiasasi variabel awal root = https://sariayu.com/ nodelist = [ root ] edgelist = [] Hasil Running :","title":"Crawl link"},{"location":"crawl_link/#crawling","text":"Web Crawler digunakan untuk mengambil data berupa teks, audio, foto bahkan video pada sebuah website.","title":"Crawling"},{"location":"crawl_link/#source-code-untuk-mengcrawl-data","text":"Proses crawling dalam suatu website dimulai dari mendata seluruh url dari website, menelusurinya satu-persatu, kemudian memasukkannya dalam daftar halaman pada indeks search engine, sehingga setiap kali ada perubahan pada website, akan terupdate secara otomatis[^1]. crawling pada halaman ini mengambil link yang ada pada sebuah web , disini saya mengambil link pada web sariayu : \" https://sariayu.com/ \" untuk mengambil link kita bisa menggunakan code dibawah ini yang sudah dijadikan sebagai target dan menambahkan https pada link. def simplifiedURL ( url ): if www. in url : ind = url . index ( www. ) + 4 url = http:// + url [ ind :] if url [ - 1 ] == / : url = url [: - 1 ] parts = url . split ( / ) url = for i in range ( 3 ): url += parts [ i ] + / return url code dibawah ini menggunakan 4 para meter antara lain : deep : mengecek kedalaman crawl show : menampilkan hasil crawl max_deep : batasan kedalaman saat proses crawl url : target alamat yang akan di crawling def crawl ( url , max_deep , show = False , deep = 0 , done = []): global edgelist deep += 1 url = simplifiedURL ( url ) if not url in done : links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( | , end = ) for i in range ( deep - 1 ): print ( -- , end = ) print ( ( %d ) %s % ( len ( links ), url )) for link in links : link = simplifiedURL ( link ) edge = ( url , link ) if not edge in edgelist : edgelist . append ( edge ) if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) selanjutnya menggunakan code dibawah ini untuk mengambil data link pada sebuah web. def getAllLinks ( src ): try : page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) tags = soup . findAll ( a ) links = [] for tag in tags : try : link = tag [ href ] if not link in links and http in link : links . append ( link ) except KeyError : pass return links except : return list () Inisialiasasi variabel awal root = https://sariayu.com/ nodelist = [ root ] edgelist = []","title":"source code untuk mengcrawl data"},{"location":"index_link/","text":"Web Crawler Menggunakan Python Nama : Husna NIM : 160411100018 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom Pengantar Program ini menggunakan Bahasa Python dengan library : pandas requests BeautifulSoup networkx matplotlib.pyplot Mengambil link di Website : \" https://sariayu.com/ \" import pandas as pd import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt","title":"Pengantar"},{"location":"index_link/#web-crawler-menggunakan-python","text":"Nama : Husna NIM : 160411100018 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom","title":"Web Crawler Menggunakan Python"},{"location":"index_link/#pengantar","text":"Program ini menggunakan Bahasa Python dengan library : pandas requests BeautifulSoup networkx matplotlib.pyplot Mengambil link di Website : \" https://sariayu.com/ \" import pandas as pd import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt","title":"Pengantar"},{"location":"note/","text":"Note Ketika ingin me-Crawl data dalam 1 link website pastinya akan menggunakan code yang berbeda. ketika ingin menjalankan code diatas secara keseluruhan dan anda menggunakan link yang berbeda dengan link yang ada dicode tersebut maka harus mengubah code pada bagian Crawl nya saja. 1 code crawl hanya dapat digunakan di 1 link website. Dan code KBI.db harus berada di 1 folder yang sama dengan file py nya. Ketika menjalankan program pastikan terhubung koneksi internet.","title":"Note"},{"location":"note/#note","text":"Ketika ingin me-Crawl data dalam 1 link website pastinya akan menggunakan code yang berbeda. ketika ingin menjalankan code diatas secara keseluruhan dan anda menggunakan link yang berbeda dengan link yang ada dicode tersebut maka harus mengubah code pada bagian Crawl nya saja. 1 code crawl hanya dapat digunakan di 1 link website. Dan code KBI.db harus berada di 1 folder yang sama dengan file py nya. Ketika menjalankan program pastikan terhubung koneksi internet.","title":"Note"},{"location":"note_link/","text":"Note Ketika ingin me-Crawl data dalam 1 link website pastinya akan menggunakan code yang berbeda. ketika ingin menjalankan code diatas secara keseluruhan dan anda menggunakan link yang berbeda dengan link yang ada dicode tersebut maka harus mengubah code pada bagian Crawl nya saja. 1 code crawl hanya dapat digunakan di 1 link website. Ketika menjalankan program pastikan terhubung koneksi internet. untuk lebih simple dan tidak perlu mendownload library satu - satu lebih baik menggunakan Spyder , karena disitu sudah lengkap library tanpa mendownload satu - satu.","title":"Note"},{"location":"note_link/#note","text":"Ketika ingin me-Crawl data dalam 1 link website pastinya akan menggunakan code yang berbeda. ketika ingin menjalankan code diatas secara keseluruhan dan anda menggunakan link yang berbeda dengan link yang ada dicode tersebut maka harus mengubah code pada bagian Crawl nya saja. 1 code crawl hanya dapat digunakan di 1 link website. Ketika menjalankan program pastikan terhubung koneksi internet. untuk lebih simple dan tidak perlu mendownload library satu - satu lebih baik menggunakan Spyder , karena disitu sudah lengkap library tanpa mendownload satu - satu.","title":"Note"},{"location":"referensi/","text":"Referensi https://www.dictio.id/t/apa-yang-dimaksud-dengan-web-crawler/15116/2 https://informatikalogi.com/algoritma-k-means-clustering/ https://repository.ipb.ac.id/handle/123456789/14020 https://informatikalogi.com/term-weighting-tf-idf/ https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/","title":"Referensi"},{"location":"referensi/#referensi","text":"https://www.dictio.id/t/apa-yang-dimaksud-dengan-web-crawler/15116/2 https://informatikalogi.com/algoritma-k-means-clustering/ https://repository.ipb.ac.id/handle/123456789/14020 https://informatikalogi.com/term-weighting-tf-idf/ https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/","title":"Referensi"},{"location":"referensi_link/","text":"Referensi https://www.dictio.id/t/apa-yang-dimaksud-dengan-web-crawler/15116/2 https://t4urusboy08.blogspot.com/ https://id.m.wikipedia.org/wiki/PageRank","title":"Referensi"},{"location":"referensi_link/#referensi","text":"https://www.dictio.id/t/apa-yang-dimaksud-dengan-web-crawler/15116/2 https://t4urusboy08.blogspot.com/ https://id.m.wikipedia.org/wiki/PageRank","title":"Referensi"},{"location":"seleksi_fitur/","text":"Seleksi Fitur source code untuk seleksi fitur, clustering dan silhouette Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur - fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur - fitur yang tidak relevan. def pearsonCalculate(data, u,v): i, j is an index atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] katadasarBaru=katadasar[:u+1] v = u while v len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar = katadasarBaru if u%50 == 0 : print( proses : , u, data.shape) u+=1 return katadasar, data katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf, 0.8) kmeans = KMeans(n_clusters=3, random_state=0).fit(fiturBaru); print(kmeans.labels_) classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print( cluster ) for i in range (2, len (fiturBaru)-1): kmeans = KMeans(n_clusters=3, random_state=0).fit(fiturBaru); classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print( Silhouete untuk , i, cluster adalah ,s_avg) print(kmeans.labels_) with open( hasil_cluster.csv , newline= , mode= w ) as employee_file: employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) for i in classnya.reshape(-1,1): employee_writer.writerow(i) with open( Seleksi_Fitur.csv , newline= , mode= w ) as employee_file: employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow([katadasarBaru.tolist()]) for i in fiturBaru: employee_writer.writerow(i) Hasil Running :","title":"Seleksi Fitur"},{"location":"seleksi_fitur/#seleksi-fitur","text":"","title":"Seleksi Fitur"},{"location":"seleksi_fitur/#source-code-untuk-seleksi-fitur-clustering-dan-silhouette","text":"Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur - fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur - fitur yang tidak relevan. def pearsonCalculate(data, u,v): i, j is an index atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] katadasarBaru=katadasar[:u+1] v = u while v len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar = katadasarBaru if u%50 == 0 : print( proses : , u, data.shape) u+=1 return katadasar, data katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf, 0.8) kmeans = KMeans(n_clusters=3, random_state=0).fit(fiturBaru); print(kmeans.labels_) classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print( cluster ) for i in range (2, len (fiturBaru)-1): kmeans = KMeans(n_clusters=3, random_state=0).fit(fiturBaru); classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print( Silhouete untuk , i, cluster adalah ,s_avg) print(kmeans.labels_) with open( hasil_cluster.csv , newline= , mode= w ) as employee_file: employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) for i in classnya.reshape(-1,1): employee_writer.writerow(i) with open( Seleksi_Fitur.csv , newline= , mode= w ) as employee_file: employee_writer = csv.writer(employee_file, delimiter= , , quotechar= , quoting=csv.QUOTE_MINIMAL) employee_writer.writerow([katadasarBaru.tolist()]) for i in fiturBaru: employee_writer.writerow(i)","title":"source code untuk seleksi fitur, clustering dan silhouette"},{"location":"seleksi_fitur/#hasil-running","text":"","title":"Hasil Running :"}]}